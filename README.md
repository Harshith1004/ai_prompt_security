# AI Prompt Security Detection System

Machine learning system to detect prompt injection and jailbreak attacks in large language models.

## ğŸ¯ Project Goal

Build a classifier that labels user prompts as:
- `0` = benign
- `1` = prompt_injection  
- `2` = jailbreak

## ğŸ“ Project Structure

```
ai_prompt_security/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # Original downloaded datasets
â”‚   â””â”€â”€ processed/    # Cleaned and labeled data
â”œâ”€â”€ notebooks/        # Jupyter notebooks for experimentation
â”œâ”€â”€ src/             # Source code
â”œâ”€â”€ models/          # Trained models
â”œâ”€â”€ results/         # Evaluation results and reports
â””â”€â”€ requirements.txt
```

## ğŸš€ Getting Started

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Mac/Linux
# venv\Scripts\activate   # On Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Download Datasets (Phase 1)

```bash
python src/download_datasets.py
```

This will download:
- **Malicious prompts**: Prompt injections, jailbreaks, red team attacks
- **Benign prompts**: ShareGPT, LMSYS chat data

### 3. Clean & Process Data

```bash
python src/data_cleaning.py
```

### 4. Train Models (Coming in Phase 3)

```bash
python src/train_model.py
```

## ğŸ“Š Dataset Sources

### Malicious Prompts
- deepset/prompt-injections
- Anthropic/hh-rlhf (red team)
- rubend18/ChatGPT-Jailbreak-Prompts

### Benign Prompts
- anon8231489123/ShareGPT_Vicuna_unfiltered
- lmsys/lmsys-chat-1m

## ğŸ”¬ Project Phases

- [x] **Phase 1**: Data Collection & Cleaning
- [ ] **Phase 2**: Feature Engineering
- [ ] **Phase 3**: Model Training
- [ ] **Phase 4**: Evaluation
- [ ] **Phase 5**: Error Analysis
- [ ] **Phase 6**: Guardrail Demo
- [ ] **Phase 7**: Dissertation

## ğŸ“ License

Research project for educational purposes.
